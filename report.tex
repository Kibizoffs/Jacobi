\documentclass[a4paper, fleqn]{report}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\usepackage{amsmath}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{tocbibind}
\usepackage{xcolor}

\geometry{top=2cm, bottom=2cm, left=2.5cm, right=2.5cm}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue
}

\titleformat{\chapter}[block]
{\normalfont\huge\bfseries}{}{0em}{}

\lstset{
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\bfseries\color{blue},
    commentstyle=\itshape\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\small,
    stepnumber=1,
    breaklines=true,
    frame=single,
    tabsize=4,
    captionpos=b,
}

\title{
\textbf{Московский Государственный Университет имени М.В.\ Ломоносова}\\
\textbf{Факультет вычислительной математики и кибернетики}\\
\textbf{Введение в численные методы}\\
Отчёт по практическому заданию
}

\author{
Студент Кибизов Кирилл, группа 207
}
\date{\number\year}

\begin{document}

\maketitle

\tableofcontents

\chapter{Постановка задачи}

\section*{Дано:}
\begin{enumerate}[label=\arabic*.]
\item Уравнение в частных производных с граничными условиями:
\[
\begin{cases}
k_x \frac{\partial^2 u}{\partial x^2} + k_y \frac{\partial^2 u}{\partial y^2} = 0, \quad (x, y) \in [0, 1] \times [0, 1], \\
u(x, 0) = 0, \quad x \in [0, 1] \\
u(0, y) = 0, \quad y \in [0, 1] \\
u(x, 1) = \sin(\pi x), \quad x \in [0, 1] \\
u(1, y) = 0, \quad y \in [0, 1] \\
\end{cases}
\]
\item Разностная схема:
\[
\begin{cases}
k_x \dfrac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + k_y \dfrac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0, \quad i = \overline{1,N-1},\ j = \overline{1,N-1}, \\
u_{i,0} = 0, \quad i = \overline{0,N}, \\
u_{0,j} = 0, \quad j = \overline{0,N}, \\
u_{i,N} = 0, \quad i = \overline{0,N}, \\
u_{N,j} = 0, \quad j = \overline{0,N}.
\end{cases}
\]
где
\[
u_{i,j} \approx u(x_i, y_j), \quad x_i = \dfrac{i}{N}, \quad y_j = \dfrac{j}{N}, \quad h = \dfrac{1}{N}.
\]
\item Аналитическое решение данной задачи:
\[
u(x, y) = \dfrac{\sinh(\pi y / \sqrt{k_y})}{\sinh(\pi / \sqrt{k_y}))} \sin(\pi x)
\]
\end{enumerate}

\section*{Задача:}
\textbf{Требуется решить данную СЛАУ с помощью итерационного метода Якоби (где он применим)} для $N = 100$, рассматривая следующие случаи:
\begin{enumerate}
    \item $k_x = k_y = 1$,
    \item $k_x = 1$, $k_y = 10^6$.
\end{enumerate}
\textbf{В случае неприменимости итерационного метода Якоби предложить рабочий альтернативный метод.}


\chapter{Описание используемых числовых методов}

\subsection*{Итерационные алгоритмы}

При применении итерационных методов решения СЛАУ $Ax = f$ ответ получается в процессе построения последовательных приближений (итераций) $x_k = \{x_1^k, x_2^k, \dots, x_n^k\}$, сходящихся к решению исходной системы в пространстве $E_n$ с евклидовой нормой $\|x\|$: $\lim_{k \to \infty} x_k = x$, где $i$ - номер компоненты, а $k$ - номер итерации.

Сходимость обеспечивает принципиальную возможность получить в процессе итераций ответ с любой наперед заданной степенью точности. 

Если очередной член последовательности $x_{k+1}$ может выражаться только через предыдущий $x_{k+1} = F(x_k) $. Такие итерационные алгоритмы называют одношаговыми. Обычно линейно одношаговые алгоритмы записывают в стандартной канонической форме:
$B_{k+1} \frac{x_{k+1} - x_k}{\tau_{k+1}} + Ax_k = f$ и $\det B_{k+1} \neq$ и $\tau_{k+1} > 0$. В такой записи процесс характеризуется последовательностью матриц $B_{k+1}$ и числовых параметров $\tau_{k+1}$, которые называют итерационными параметрами.

\subsection*{Метод Якоби}
Метод Якоби является простым итерационным методом решения системы линейных уравнений, основанным на разностной схеме. В этом методе значения функции в каждой точке сетки обновляются независимо, используя значения с предыдущей итерации. Метод Якоби можно записать в виде:
\[
u_{i,j}^{(k+1)} = \frac{k_x (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)}) + k_y (u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})}{2(k_x + k_y)},
\]
где \( u_{i,j}^{(k)} \) — значение функции в точке \((i, j)\) на \( k \)-й итерации, а \( k_x \) и \( k_y \) — коэффициенты.

Метод Якоби обладает сравнительно низкой скоростью сходимости, поскольку обновления в каждой точке сетки не зависят от результатов на предыдущих итерациях. Это может привести к большому количеству итераций для достижения заданной точности. С увеличением числа итераций погрешность метода уменьшается, притом увеличивается вычислительная сложность алгоритма.

\subsection*{Метод верхней релаксации (SOR)}
Метод верхней релаксации (SOR) является модификацией метода Гаусса-Зейделя, в котором значения функции обновляются с дополнительной корректировкой с использованием параметра релаксации \( \omega \). Этот параметр регулирует скорость сходимости метода. Итерационная схема для метода SOR выглядит следующим образом:
\[
u_{i,j}^{(k+1)} = (1 - \omega) u_{i,j}^{(k)} + \omega \left( \frac{k_x (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)}) + k_y (u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})}{2(k_x + k_y)} \right),
\]
где \( \omega \) — параметр релаксации, принимающий значения от 1 до 2. Параметр \( \omega \) регулирует скорость сходимости итераций: при оптимальном выборе \( \omega \) можно добиться минимального числа итераций для достижения заданной точности.

Метод SOR, в отличие от метода Якоби, сразу использует данные, обновлённые во время текущей итерации. Выбор значения параметра \( \omega \) оказывает значительное влияние на минимизацию погрешности и скорость сходимости.


\chapter{Анализ применимости используемых числовых методов}

Перед тем как применять итерационные методы для решения системы линейных алгебраических уравнений (СЛАУ), необходимо убедиться, что они сходятся в рассматриваемом случае. Это включает в себя проверку структуры и свойств матрицы системы, а также оценку выполнения достаточных условий сходимости итерационных методов.

\section*{Достаточные условия сходимости итерационного процесса}

\subsection*{Самосопряжённость матрицы}
В одномерном случае в направлении $x$ вторая производная $\frac{\partial^2 u}{\partial x^2}$ аппроксимируется по формуле:
\[
\frac{\partial^2 u}{\partial x^2} \approx \frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h_x^2}.
\]
Важно, что коэффициенты при $u_{i+1,j}$ и $u_{i-1,j}$ одинаковы (оба равны $\frac{1}{h_x^2}$).  
Аналогично производится аппроксимация второй производной по оси $y$.
Благодаря симметрии разностной схемы все связи между узлами в матрице $A$ получаются парными и "зеркальными". Это говорит о симметричности матрицы. Известные значения на границах области не делают матрицу несимметричной, так как они просто выносятся в вектор правой части $f$. Таким образом, можно сделать вывод, что матрица $A$ — самосопряжённая. В случае вещественной матрицы (все элементы матрицы $A$ — вещественные) понятия самосопряжённости и симметричности совпадают.

\subsection*{Положительно определённая матрица}
Чтобы доказать, что матрица $A$ положительно определённая, нужно показать, что для любого ненулевого вектора $v$ выполняется неравенство: $v^T A v > 0$.
Рассмотрим выражение $v^T A v$. Это скаляр, который можно записать как:
\[
v^T A v = \sum_{i,j} v_{i,j} (Av)_{i,j}.
\]
Для матрицы $A$, полученной из разностной аппроксимации второго порядка, можно записать, что $A$ действует на вектор $v$ следующим образом:
\[
(Av)_{i,j} = \frac{k_x}{h^2}(v_{i+1,j} - 2v_{i,j} + v_{i-1,j}) + \frac{k_y}{h^2}(v_{i,j+1} - 2v_{i,j} + v_{i,j-1}).
\]
Подставим это в $v^T A v$ и раскроем сумму:
\[
v^T A v = \sum_{i,j} v_{i,j} \left( \frac{k_x}{h^2}(v_{i+1,j} - 2v_{i,j} + v_{i-1,j}) + \frac{k_y}{h^2}(v_{i,j+1} - 2v_{i,j} + v_{i,j-1}) \right).
\]
При раскрытии суммы оказывается, что многие члены сокращаются. Получаем:
\[
v^T A v = \sum_{i,j} \frac{k_x}{h^2} (v_{i+1,j} - v_{i,j})^2 + \frac{k_y}{h^2} (v_{i,j+1} - v_{i,j})^2.
\]
В выражении $v^T A v$ остались только суммы квадратов разностей значений $v$ в соседних узлах. Так как $k_x > 0$, $k_y > 0$ и $h > 0$, каждый член суммы неотрицателен. Так как $v^T A v$ является суммой строго неотрицательных слагаемых, и каждое из них положительно, если $v \neq 0$, то $v^T A v > 0$. Это доказывает, что матрица $A$ положительно определённая.

\subsection*{Теорема Самарского}
Пусть $A$ — самосопряжённая положительно определённая матрица:  
$A = A^T, \quad A > 0,$  
и $B = A - \frac{\tau}{2} A$ — положительно определённая матрица, $\tau$ — положительное число:  
$B = A - \frac{\tau}{2} A > 0.$

Можно утверждать, что для матрицы $A$, которая:
\begin{itemize}
    \item симметрична ($A^T = A$),
    \item положительно определённая ($v^T A v > 0$ для любого $v \neq 0$),
\end{itemize}
выполняются достаточные условия сходимости итерационных методов, таких как методы Якоби и верхней релаксации (SOR). 

\subsection*{Сходимость методов}
В итоге методы Якоби и верхней релаксации (SOR) применимы, к данной задаче, однако важно также учитывать:
\begin{itemize}
    \item \textbf{Точность решения:} Точность определяется выбранным критерием остановки (например, достижением малого значения невязки или изменения решения между итерациями).
    \item \textbf{Быстрота сходимости:} Для улучшения быстроты сходимости можно:
    \begin{itemize}
        \item уменьшить шаг $h$,
        \item использовать "ускоряющие" параметры, такие как $\omega$ в методе верхней релаксации,
    \end{itemize}
\end{itemize}
\newpage


\chapter{Результаты}

\section*{}
В данном разделе представлены результаты численного решения уравнения с использованием метода Якоби для одного из тестов. \\
Для каждой точки вычислены значения, полученные численно и аналитически, с целью сравнения точности решения. \\
Пример вывода для первого теста представлен ниже. \\
1ый столбец - координаты точек, \\
2ой столбец - численное рещение, \\
3ий столбец - аналитическое решение, \\

\lstset{language=bash}
\begin{lstlisting}[title={Вывод метода Якоби дл 1го теста (для некоторых точек, покоординатно кратных 0.25)}]
./a.out
Please, input amount of tests (max 10): 1
Leave 3rd argument as 0 (for Jacobi) or as w (w = 1 for Gauss-Seidel; 1 < w < 2 for SOR)
Input kx and ky and w; for test #1: 1 1 0
...
u(0.250000, 0.500000) | 0.139489 | 0.140904
u(0.250000, 0.750000) | 0.319105 | 0.320099
u(0.250000, 1.000000) | 0.707107 | 0.707107
u(0.500000, 0.000000) | 0.000000 | 0.000000
...
u(0.500000, 0.750000) | 0.451283 | 0.452688
u(0.500000, 1.000000) | 1.000000 | 1.000000
u(0.750000, 0.000000) | 0.000000 | 0.000000
u(0.750000, 0.250000) | 0.052184 | 0.053187
...
--------
Test #1:
Iterations = 10247
\end{lstlisting} 

Для других тестов результаты можно получить вручную, используя программу, с различными значениями параметра \(\omega\), чтобы исследовать его влияние на сходимость метода. \\

Кроме того, для метода SOR был найден оптимальный параметр \(\omega\) = 1.94, который обеспечил наилучшую сходимость.


\chapter*{Заключение}
\addcontentsline{toc}{chapter}{Заключение}

\begin{table}[h!]
\centering
\begin{tabular}{|l|p{7cm}|p{7cm}|}
\hline
\textbf{Критерий} & \textbf{Метод Якоби} & \textbf{Метод SOR} \\ \hline
\textbf{Реализация} & 
\begin{tabular}[c]{@{}l@{}}
- Два массива, поэтапное обновление. \\ 
- В каждой итерации элементы независимы. \\ 
- Без вспомогательных параметров.
\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}
- Один массив, обновление "на ходу". \\ 
- В каждой итерации элементы зависимы. \\ 
- Требуется параметр \(\omega\) \\ 
  (0 < \(\omega\) \(\leq\) 2), влияющий на сходимость.
\end{tabular} \\ \hline
\textbf{Сходимость} & Медленнее & Существенно быстрее \\ \hline
\textbf{Погрешность} & Больше & Существенно меньше \\ \hline
\end{tabular}
\caption{Сравнение метода Якоби и метода SOR}
\end{table}


В данном отчёте была рассмотрена задача решения уравнения в частных производных с помощью итерационных методов, таких как метод Якоби и метод верхней релаксации. Реализация методов была выполнена на языке программирования C. Были получены результаты со сравнимо высокой точностью относительно предложенного аналитического решения.

\chapter*{Приложения}
\addcontentsline{toc}{chapter}{Приложения}

\href{https://github.com/kibizoffs/jacobi}{https://github.com/kibizoffs/jacobi} \\

Программа \texttt{main.c} решает задачу с использованием методов Якоби и SOR. \\
Изображения графиков, полученные в процессе вычислений, сохраняются в директории \texttt{media}. \\

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/best_w_1_1.png}
        \caption{Best \(\omega\) (kx = 1, ky = 1), \(\omega\) = 1.94}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/best_w_1_1000000.png}
        \caption{Best \(\omega\) (kx = 1, ky = $10^6$), \(\omega\) = 1.94}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/jacobi_1_1.png}
        \caption{Jacobi (kx = 1, ky = 1), iters = 10247}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/jacobi_1_1000000.png}
        \caption{Jacobi (kx = 1, ky = $10^6$), iters = 13029}
    \end{minipage}
    \par\bigskip
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/dif_jacobi_1_1.png}
        \caption{Dif Jacobi (kx = 1, ky = 1)}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/dif_jacobi_1_1000000.png}
        \caption{Dif Jacobi (kx = 1, ky = $10^6$)}
    \end{minipage}
    \par\bigskip
\end{figure*}

\newpage
\begin{figure*}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/best_res_1_1.png}
        \caption{Best res (kx = 1, ky = 1), iters = 229}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/best_res_1_1000000.png}
        \caption{Best res (kx = 1, ky = $10^6$), iters = 245}
    \end{minipage}
    \par\bigskip
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/best_dif_1_1.png}
        \caption{Best dif (kx = 1, ky = 1)}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{media/best_dif_1_1000000.png}
        \caption{Best dif (kx = 1, ky = $10^6$)}
    \end{minipage}
\end{figure*}


\begin{thebibliography}{2}
\renewcommand{\bibname}{Литература}
\bibitem{kostomarov}
Костомаров Д.\,П., Фаворский А.\,П. \textit{Вводные лекции по численным методам}. — М.: Логос, 2004. — 184 с.
\bibitem{samarskiy}
Самарский А.\,А. \textit{Введение в численные методы}. — М.: Наука, 1989. — 416 с.
\end{thebibliography}

\end{document}
